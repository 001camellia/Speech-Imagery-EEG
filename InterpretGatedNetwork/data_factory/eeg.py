# data_provider/eeg.py
import os
import mne
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
import json
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# åœ¨ eeg.py çš„é¡¶éƒ¨ä¿®æ”¹å¯¼å…¥
import os
import mne
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
import json
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# ä¿®æ”¹è¿™éƒ¨åˆ†å¯¼å…¥ä»£ç 
try:
    from .eeg_processor import (
        process_imagine_fif_data_with_label_mapping,
        load_text_maps,
        find_imagine_fif_files,
        validate_eeg_data,
        verify_data_shape_and_type,
        map_text_labels_to_numeric,
        create_3category_mapping,
        convert_to_3category_labels
    )
    print("âœ“ æˆåŠŸå¯¼å…¥ eeg_processor æ¨¡å—")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ preprocess_eeg_data_with_downsampling
    try:
        from .eeg_processor import preprocess_eeg_data_with_downsampling
        # åˆ›å»ºåˆ«å
        preprocess_eeg_data = preprocess_eeg_data_with_downsampling
        print("âœ“ æˆåŠŸå¯¼å…¥ preprocess_eeg_data_with_downsampling å¹¶åˆ›å»ºåˆ«å")
    except ImportError:
        print("âš  æ³¨æ„: eeg_processorä¸­æ²¡æœ‰preprocess_eeg_data_with_downsamplingå‡½æ•°")
        preprocess_eeg_data = None
        
except ImportError as e:
    print(f"âš  è­¦å‘Š: å¯¼å…¥ eeg_processor æ¨¡å—å¤±è´¥: {e}")
    print("âš  å°†ä½¿ç”¨æœ¬åœ°å‡½æ•°å®šä¹‰")
    
    # åœ¨æœ¬åœ°å®šä¹‰å‡½æ•°
    def process_imagine_fif_data_with_label_mapping(*args, **kwargs):
        raise ImportError("eeg_processor æ¨¡å—æœªæ‰¾åˆ°")
    
    def convert_to_3category_labels(numeric_labels):
        """å°†39ç±»æ ‡ç­¾è½¬æ¢ä¸º3ç±»æ ‡ç­¾"""
        mapping_3cat = create_3category_mapping()
        new_labels = [mapping_3cat.get(label, -1) for label in numeric_labels]
        return new_labels
    
    def create_3category_mapping():
        """3åˆ†ç±»æ˜ å°„ï¼šæ—¥å¸¸ç”Ÿæ´»(0) vs ç¤¾äº¤æƒ…æ„Ÿ(1) vs ä¸“ä¸šæœåŠ¡(2)"""
        return {
            0: 0, 13: 0, 14: 0, 18: 0, 22: 0, 23: 0, 26: 0, 35: 0, 37: 0,  # æ—¥å¸¸ç”Ÿæ´»
            1: 1, 2: 1, 6: 1, 7: 1, 9: 1, 12: 1, 15: 1, 17: 1, 24: 1, 29: 1, 34: 1, 36: 1, 38: 1,  # ç¤¾äº¤æƒ…æ„Ÿ
            3: 2, 4: 2, 5: 2, 8: 2, 10: 2, 11: 2, 16: 2, 19: 2, 20: 2, 21: 2, 25: 2, 27: 2, 28: 2, 30: 2, 31: 2, 32: 2, 33: 2  # ä¸“ä¸šæœåŠ¡
        }

# å¯¼å…¥ç°æœ‰çš„æ•°æ®å¤„ç†å·¥å…·
from .uea import Normalizer, interpolate_missing, subsample


def eeg_collate_fn(data, max_len=None):
    """
    EEGä¸“ç”¨collateå‡½æ•°
    æ”¯æŒåŠ¨æ€é•¿åº¦
    """
    batch_size = len(data)
    features, labels = zip(*data)
    
    # è·å–åºåˆ—é•¿åº¦
    seq_len = features[0].shape[0]  # è½¬ç½®åæ˜¯(seq_len, feat_dim)
    
    # ç›´æ¥stackï¼Œå› ä¸ºEEGæ˜¯å›ºå®šé•¿åº¦
    X = torch.stack(features, dim=0)  # (batch_size, seq_len, feat_dim)
    targets = torch.stack(labels, dim=0)  # (batch_size,)
    
    # åˆ›å»ºå…¨1çš„padding_mask
    padding_masks = torch.ones(batch_size, seq_len, dtype=torch.bool)
    
    return X, targets, padding_masks

class EEGDataset(Dataset):
    """
    EEGæƒ³è±¡ä»»åŠ¡æ•°æ®é›†ï¼Œ39åˆ†ç±»ç‰ˆæœ¬
    é€‚é…ç°æœ‰æ¡†æ¶çš„æ•°æ®é›†æ¥å£
    """
    def __init__(self, root_path, flag='train', size=None, features='S', 
                 data_path='', target='OT', scale=True, timeenc=0, freq='h', 
                 seasonal_patterns=None, nbins=10, bin_edges=None, 
                 json_path=None, max_files=10, debug=False, 
                 test_size=0.2, val_size=0.1, random_seed=42,args=None):
        
        """
        Args:
            root_path: æ•°æ®æ ¹ç›®å½•
            flag: 'train', 'val', 'test'
            size: [seq_len, label_len, pred_len] (åˆ†ç±»ä»»åŠ¡ç”¨ä¸åˆ°label_lenå’Œpred_len)
            json_path: textmaps.jsonè·¯å¾„
            max_files: æœ€å¤§å¤„ç†æ–‡ä»¶æ•°
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            val_size: éªŒè¯é›†æ¯”ä¾‹
        """
        try:
            # å¤„ç†flagå¤§å°å†™
            if isinstance(flag, str):
                flag = flag.lower()
                if flag == 'validation':
                    flag = 'val'

            if flag not in ['train', 'val', 'test']:
                if debug:
                    print(f"âš  è­¦å‘Š: flag={flag} ä¸æ˜¯æ ‡å‡†å€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'train'")
                flag = 'train'

            # ä¿å­˜å‚æ•°
            self.flag = flag
            self.seq_len = None  # ä¸å†ç¡¬ç¼–ç 
            self.label_len = size[1] if size else 0
            self.pred_len = size[2] if size else 0
            self.scale = scale
            self.json_path = json_path
            self.max_files = max_files
            self.debug = debug
            self.test_size = test_size
            self.val_size = val_size
            self.random_seed = random_seed
            
            # === å›ºå®šå‚æ•° ===
            self.original_fs = 500  # å›ºå®š: åŸå§‹é‡‡æ ·ç‡500Hz
            self.target_fs = 256    # å›ºå®š: ç›®æ ‡é‡‡æ ·ç‡256Hz
            self.target_channels = 122  # å›ºå®š: ç›®æ ‡é€šé“æ•°122
            self.downsample_method = 'decimate'  # å›ºå®š: ä¸‹é‡‡æ ·æ–¹æ³•
            self.target_timepoints = None  # ç¨åä»æ•°æ®è®¡ç®—

            # è®¾ç½®éšæœºç§å­
            np.random.seed(random_seed)
            torch.manual_seed(random_seed)

            if debug:
                print(f"\n{'='*60}")
                print("EEGDatasetåˆå§‹åŒ–")
                print(f"å‚æ•°:")
                print(f"  flag: {flag}")
                print(f"  root_path: {root_path}")
                print(f"  target_fs: {self.target_fs}Hz")
                print(f"  json_path: {json_path}")
                print(f"  original_fs: {self.original_fs}Hz (å›ºå®š)")
                print(f"  target_fs: {self.target_fs}Hz (å›ºå®š)")
                print(f"  target_channels: {self.target_channels}")

            # éªŒè¯å¿…è¦å‚æ•°
            if json_path is None or not os.path.exists(json_path):
                raise FileNotFoundError(f"textmaps.jsonæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")

            # åŠ è½½å’Œå¤„ç†EEGæ•°æ®
            print("æ­£åœ¨åŠ è½½EEGæ•°æ®...")
            self.data_dict = self._load_eeg_data(root_path)
            if not self.data_dict:
                raise ValueError("æ— æ³•åŠ è½½EEGæ•°æ®")
            self.samples = self._prepare_samples()
            print(f"æœ‰æ•ˆæ ·æœ¬æ•°é‡: {len(self.samples)}")
            # ä»data_dictè·å–å®é™…å°ºå¯¸
            if 'input_features' in self.data_dict and len(self.data_dict['input_features']) > 0:
                # è·å–å®é™…å½¢çŠ¶
                n_samples, n_channels, n_times = self.data_dict['input_features'].shape
                self.target_timepoints = n_times
                self.target_channels = n_channels

                # æ›´æ–°seq_len
                if size and size[0] is not None:
                    self.seq_len = size[0]
                else:
                    self.seq_len = n_times

                # æ·»åŠ max_seq_lenå±æ€§
                self.max_seq_len = n_times

                if self.debug:
                    print(f"å®é™…æ•°æ®å½¢çŠ¶: {self.data_dict['input_features'].shape}")
                    print(f"seq_lenè®¾ç½®ä¸º: {self.seq_len}")
                    print(f"max_seq_len: {self.max_seq_len}")
            else:
                # ä½¿ç”¨é»˜è®¤å€¼
                if size and size[0] is not None:
                    self.seq_len = size[0]
                else:
                    # è®¡ç®—é»˜è®¤å€¼: 1651/500 â‰ˆ 3.302ç§’ Ã— 256Hz â‰ˆ 845
                    self.seq_len = int(1651 * self.target_fs / self.original_fs)
                self.max_seq_len = self.seq_len
                print(f"âš  è­¦å‘Š: æ— æ³•ä»data_dictè·å–å°ºå¯¸ï¼Œä½¿ç”¨é»˜è®¤å€¼: {self.seq_len}")

            if self.debug:
                print(f"âœ“ è®¾ç½®max_seq_len: {self.max_seq_len}")
                print("âœ“ æ•°æ®é›†åˆ›å»ºå®Œæˆ")
                if 'input_features' in self.data_dict:
                    print(f"  è¾“å…¥ç‰¹å¾å½¢çŠ¶: {self.data_dict['input_features'].shape}")
                if 'numeric_labels' in self.data_dict:
                    print(f"  æ ‡ç­¾å½¢çŠ¶: {self.data_dict['numeric_labels'].shape}")
                if 'num_classes' in self.data_dict:
                    print(f"  ç±»åˆ«æ•°é‡: {self.data_dict['num_classes']}")

            # æ•°æ®æ ‡å‡†åŒ–
            if self.scale:
                self._setup_normalizer()

            # æ•°æ®åˆ’åˆ†
            self.samples = self._split_samples_by_flag()

            if self.debug and len(self.samples) > 0:
                print(f"\nâœ“ EEGDataset ({flag}é›†) åˆå§‹åŒ–å®Œæˆ:")
                print(f"  æ ·æœ¬æ•°é‡: {len(self.samples)}")
                sample = self.samples[0]
                print(f"  ç‰¹å¾å½¢çŠ¶: {sample['features'].shape}")
                print(f"  ç±»åˆ«æ•°é‡: {self.data_dict.get('num_classes', 'unknown')}")
                print(f"  seq_len: {self.seq_len}")
                print(f"  max_seq_len: {self.max_seq_len}")

        except Exception as e:
            print(f"\nâŒ EEGDatasetåˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _load_eeg_data(self, data_dir):
        """åŠ è½½EEGæ•°æ®"""
        if self.debug:
            print(f"æ­£åœ¨åŠ è½½EEGæ•°æ®")
            print(f"  æ•°æ®ç›®å½•: {data_dir}")
            print(f"  JSONè·¯å¾„: {self.json_path}")
            print(f"  æœ€å¤§æ–‡ä»¶æ•°: {self.max_files}")
            print(f"  å›ºå®šé‡‡æ ·ç‡: {self.original_fs}Hz -> {self.target_fs}Hz")
            print(f"  ä¸‹é‡‡æ ·å› å­: {self.original_fs/self.target_fs:.1f}")
            # ä»self.argsä¸­è·å–subject_idså‚æ•°
        subject_ids = getattr(self.args, 'subject_ids', None)
    
        if self.debug and subject_ids:
            print(f"  æŒ‡å®šè¢«è¯•: {subject_ids}")
        # æ£€æŸ¥æ•°æ®ç›®å½•
        if not os.path.exists(data_dir):
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")

            # å°è¯•æŸ¥æ‰¾æ›¿ä»£è·¯å¾„
            print(f"ğŸ” å°è¯•æŸ¥æ‰¾æ›¿ä»£è·¯å¾„...")
            possible_paths = [
                data_dir,
                "/root/autodl-tmp/InterpretGatedNetwork-main/data",
                "/root/autodl-tmp/InterpretGatedNetwork-main/data/imagine",
                "/root/autodl-tmp/InterpretGatedNetwork-main/datasets",
                "/root/autodl-tmp/InterpretGatedNetwork-main"
            ]

            for path in possible_paths:
                if os.path.exists(path):
                    print(f"  âœ“ æ‰¾åˆ°å­˜åœ¨è·¯å¾„: {path}")
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•
                    if os.path.isdir(path):
                        # åˆ—å‡ºå†…å®¹
                        contents = os.listdir(path)[:10]  # å‰10ä¸ª
                        print(f"    ç›®å½•å†…å®¹: {contents}")
                    # æ›´æ–°data_dir
                    data_dir = path
                    print(f"  â†’ ä½¿ç”¨è·¯å¾„: {data_dir}")
                    break

        if not os.path.exists(data_dir):
            return None

        # æ£€æŸ¥JSONæ–‡ä»¶
        if not os.path.exists(self.json_path):
            print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {self.json_path}")
            # å°è¯•æŸ¥æ‰¾jsonæ–‡ä»¶
            json_candidates = [
                self.json_path,
                "/root/autodl-tmp/InterpretGatedNetwork-main/data/textmaps.json",
                "/root/autodl-tmp/InterpretGatedNetwork-main/datasets/textmaps.json",
                "/root/autodl-tmp/InterpretGatedNetwork-main/textmaps.json"
            ]

            for json_path in json_candidates:
                if os.path.exists(json_path):
                    print(f"  âœ“ æ‰¾åˆ°JSONæ–‡ä»¶: {json_path}")
                    self.json_path = json_path
                    break

            if not os.path.exists(self.json_path):
                return None

        try:
            data_dict = process_imagine_fif_data_with_label_mapping(
                data_dir, 
                self.json_path, 
                self.max_files, 
                debug=self.debug,
                target_channels=self.target_channels,
                target_timepoints=self.target_timepoints,
                original_fs=self.original_fs,  # ä½¿ç”¨å›ºå®šçš„åŸå§‹é‡‡æ ·ç‡
                target_fs=self.target_fs,  # ç›®æ ‡é‡‡æ ·ç‡
                downsample_method=self.downsample_method
            )

            if data_dict is None:
                print("âŒ process_imagine_fif_data_with_label_mapping è¿”å› None")
                return None

            print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"  æ ·æœ¬æ•°: {len(data_dict.get('input_features', []))}")
            print(f"  å®é™…å½¢çŠ¶: {data_dict.get('input_features', torch.tensor([])).shape}")

            return data_dict

        except Exception as e:
            print(f"âŒ è°ƒç”¨process_imagine_fif_data_with_label_mappingå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


    
    def _setup_normalizer(self):
        """è®¾ç½®æ ‡å‡†åŒ–å™¨"""
        # EEGæ•°æ®æ ‡å‡†åŒ–ç­–ç•¥ï¼šæŒ‰æ ·æœ¬æ ‡å‡†åŒ–
        self.normalizer = Normalizer(norm_type='per_sample_std')
        
        # å¦‚æœæœ‰éœ€è¦ï¼Œå¯ä»¥é¢„å…ˆè®¡ç®—å…¨å±€ç»Ÿè®¡é‡
        if 'input_features' in self.data_dict:
            all_features = self.data_dict['input_features']  # (n_samples, 122, 1651)
            
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼ä»¥ä¾¿ä½¿ç”¨Normalizer
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å°†3Dæ•°æ®è½¬æ¢ä¸º2Dæ ¼å¼
            n_samples, n_channels, n_times = all_features.shape
            features_2d = all_features.reshape(-1, n_times)  # (n_samples*n_channels, n_times)
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(features_2d.numpy())
            
            # è®¡ç®—æ ‡å‡†åŒ–å‚æ•°
            self.normalizer.normalize(df)
    
    def _normalize_sample(self, features):
        """å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæ ‡å‡†åŒ–"""
        if not self.scale or not hasattr(self, 'normalizer'):
            return features
        
        # å°†ç‰¹å¾è½¬æ¢ä¸ºDataFrame
        n_channels, n_times = features.shape
        df = pd.DataFrame(features.numpy())
        
        # ä½¿ç”¨Normalizerè¿›è¡Œæ ‡å‡†åŒ–
        normalized_df = self.normalizer.normalize(df)
        
        # è½¬æ¢å›tensor
        normalized_tensor = torch.tensor(normalized_df.values, dtype=torch.float32)
        
        return normalized_tensor
    
    '''def _split_samples_by_flag(self):
        """æ ¹æ®flagåˆ’åˆ†æ•°æ®é›†"""
        all_samples = self._prepare_samples()
        n_samples = len(all_samples)
        
        if n_samples == 0:
            return []
        
        # è®¡ç®—åˆ’åˆ†ç‚¹
        n_test = int(n_samples * self.test_size)
        n_val = int(n_samples * self.val_size)
        n_train = n_samples - n_test - n_val
        
        # æ‰“ä¹±ç´¢å¼•
        indices = np.random.permutation(n_samples)
        
        # åˆ’åˆ†ç´¢å¼•
        train_indices = indices[:n_train]
        val_indices = indices[n_train:n_train + n_val]
        test_indices = indices[n_train + n_val:]
        
        # æ ¹æ®flagé€‰æ‹©
        if self.flag == 'train':
            selected_indices = train_indices
        elif self.flag == 'val':
            selected_indices = val_indices
        elif self.flag == 'test':
            selected_indices = test_indices
        else:
            raise ValueError(f"æ— æ•ˆçš„flag: {self.flag}")
        
        # é€‰æ‹©æ ·æœ¬
        selected_samples = [all_samples[i] for i in selected_indices]
        
        if self.debug:
            print(f"\næ•°æ®åˆ’åˆ†:")
            print(f"  æ€»å…±æ ·æœ¬: {n_samples}")
            print(f"  Train: {len(train_indices)} ä¸ª")
            print(f"  Val:   {len(val_indices)} ä¸ª")
            print(f"  Test:  {len(test_indices)} ä¸ª")
            print(f"  {self.flag}: {len(selected_samples)} ä¸ª")
        
        return selected_samples'''
    def _split_samples_by_flag(self):
        """æ ¹æ®flagåˆ’åˆ†æ•°æ®é›†"""
        all_samples = self._prepare_samples()
        n_samples = len(all_samples)
        print(f"\næ•°æ®åˆ’åˆ†:")
        print(f"  æ€»æ ·æœ¬æ•°: {n_samples}")
        print(f"  æµ‹è¯•é›†æ¯”ä¾‹: {self.test_size}")
        print(f"  éªŒè¯é›†æ¯”ä¾‹: {self.val_size}")
        if n_samples == 0:
            if self.debug:
                print(f"âš  è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ ·æœ¬")
            return []

        # ç¡®ä¿éªŒè¯é›†æ¯”ä¾‹ä¸è¶…è¿‡æœ€å¤§æ ·æœ¬æ•°
        n_val = int(n_samples * self.val_size)
        n_test = int(n_samples * self.test_size)
        n_train = n_samples - n_val - n_test

        # ç¡®ä¿æ¯ä¸ªåˆ†åŒºè‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬
        if n_train < 1:
            n_train = 1
            n_val = min(n_samples - 1, n_val)
            n_test = n_samples - n_train - n_val
        elif n_val < 1 and n_samples > 1:
            n_val = 1
            n_test = min(n_samples - n_train - 1, n_test)
            n_train = n_samples - n_val - n_test

        if self.debug:
            print(f"æ•°æ®åˆ’åˆ†:")
            print(f"  æ€»å…±æ ·æœ¬: {n_samples}")
            print(f"  Train: {n_train} ä¸ª")
            print(f"  Val:   {n_val} ä¸ª")
            print(f"  Test:  {n_test} ä¸ª")

        # æ‰“ä¹±ç´¢å¼•
        indices = np.random.permutation(n_samples)

        # åˆ’åˆ†ç´¢å¼•
        train_indices = indices[:n_train]
        val_indices = indices[n_train:n_train + n_val] if n_val > 0 else []
        test_indices = indices[n_train + n_val:] if n_test > 0 else []

        # æ ¹æ®flagé€‰æ‹©
        if self.flag == 'train':
            selected_indices = train_indices
        elif self.flag == 'val':
            selected_indices = val_indices
        elif self.flag == 'test':
            selected_indices = test_indices
        else:
            raise ValueError(f"æ— æ•ˆçš„flag: {self.flag}")

        if self.debug:
            print(f"  {self.flag}é›†: {len(selected_indices)} ä¸ªæ ·æœ¬")

        # é€‰æ‹©æ ·æœ¬
        selected_samples = [all_samples[i] for i in selected_indices] if len(selected_indices) > 0 else []

        return selected_samples
    
    def _prepare_samples(self):
        """å‡†å¤‡æ ·æœ¬åˆ—è¡¨"""
        samples = []
        if self.data_dict and 'input_features' in self.data_dict:
            input_features = self.data_dict['input_features']
            numeric_labels = self.data_dict['numeric_labels']
            text_labels = self.data_dict.get('text_labels', ['unknown'] * len(input_features))
            
            for i in range(len(input_features)):
                features = input_features[i]  # (122, 1651)
                label = numeric_labels[i]
                text_label = text_labels[i] if i < len(text_labels) else 'unknown'
                
                # æ ‡å‡†åŒ–
                if self.scale:
                    features = self._normalize_sample(features)
                else:
                    features = features.float()
                
                sample = {
                    'features': features,      # (122, 1651)
                    'label': torch.tensor(label, dtype=torch.long),  # æ•°å­—æ ‡ç­¾
                    'text_label': text_label  # æ–‡æœ¬æ ‡ç­¾
                }
                samples.append(sample)
        
        return samples
    
    def __getitem__(self, index):
        """è·å–å•ä¸ªæ ·æœ¬"""
        sample = self.samples[index]
        
        # è·å–ç‰¹å¾å’Œæ ‡ç­¾
        seq_x = sample['features']  # (122, 1651)
        target = sample['label']    # æ•°å­—æ ‡ç­¾
        
        # è½¬ç½®ä¸ºæ¡†æ¶æœŸæœ›çš„æ ¼å¼: (seq_len, feat_dim) = (1651, 122)
        # è¿™æ˜¯ä¸ºäº†ä¸å…¶ä»–æ•°æ®é›†ä¿æŒä¸€è‡´
        seq_x = seq_x.transpose(0, 1)
        
        return seq_x, target
    
    def __len__(self):
        return len(self.samples)
    
    def inverse_transform(self, data):
        """é€†å˜æ¢ï¼ˆå¦‚éœ€è¦ï¼‰"""
        if not self.scale or not hasattr(self, 'normalizer'):
            return data
        # TODO: å®ç°é€†æ ‡å‡†åŒ–
        return data
    
    def get_class_distribution(self):
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        labels = [sample['label'].item() for sample in self.samples]
        label_counts = Counter(labels)
        
        distribution = {}
        for label_id, count in sorted(label_counts.items()):
            percentage = count / len(self.samples) * 100
            distribution[label_id] = {'count': count, 'percentage': percentage}
        
        return distribution
    
    def get_sample_info(self, index):
        """è·å–æ ·æœ¬è¯¦ç»†ä¿¡æ¯"""
        if index >= len(self.samples):
            raise IndexError(f"ç´¢å¼•è¶…å‡ºèŒƒå›´: {index}")
        
        sample = self.samples[index]
        return {
            'features_shape': sample['features'].shape,
            'label': sample['label'].item(),
            'text_label': sample['text_label'],
            'features_stats': {
                'min': sample['features'].min().item(),
                'max': sample['features'].max().item(),
                'mean': sample['features'].mean().item(),
                'std': sample['features'].std().item()
            }
        }


class EEGDataset3Class(EEGDataset):
    """3åˆ†ç±»ç‰ˆæœ¬çš„EEGæ•°æ®é›†"""
    def __init__(self, root_path, flag='train', size=None, features='S', 
                 data_path='', target='OT', scale=True, timeenc=0, freq='h', 
                 seasonal_patterns=None, nbins=10, bin_edges=None, 
                 json_path=None, max_files=10, debug=False, 
                 test_size=0.2, val_size=0.1, random_seed=42,subject_ids=None,args=None):
        
        # å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            root_path, flag, size, features, data_path, target, scale, 
            timeenc, freq, seasonal_patterns, nbins, bin_edges, 
            json_path, max_files, debug, test_size, val_size, random_seed
           
        )
        # ä¿å­˜args
        self.args = args
        
        # ä»argsä¸­è·å–subject_ids
        if subject_ids is None and args is not None:
            subject_ids = getattr(args, 'subject_ids', None)
        
        # ä¿å­˜subject_idså‚æ•°
        self.subject_ids = subject_ids
        
        if debug and subject_ids:
            print(f"EEGDataset3Class: å¤„ç†è¢«è¯•: {subject_ids}")
        
        # è½¬æ¢ä¸º3åˆ†ç±»
        self._convert_to_3class()
        
        if self.debug:
            print(f"âœ“ EEGDataset3Class ({flag}é›†) åˆå§‹åŒ–å®Œæˆ")
            print(f"  æ ·æœ¬æ•°é‡: {len(self.samples)}")
            print(f"  ç±»åˆ«æ•°é‡: 3")
     # === æ–°å¢: è½¬æ¢ä¸ºDataFrameæ ¼å¼ ===
        #self._convert_to_dataframe_format()
       
        
    '''def _convert_to_dataframe_format(self):
        """å°†3D tensoræ•°æ®è½¬æ¢ä¸ºDataFrameæ ¼å¼"""
        import pandas as pd
        import numpy as np
        
        if not self.samples or not hasattr(self, 'data_dict'):
            return
        
        # ä»samplesé‡å»ºtensor
        n_samples = len(self.samples)
        if n_samples == 0:
            return
        
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å½¢çŠ¶
        sample_feat = self.samples[0]['features']  # (122, 1651)
        n_channels, n_times = sample_feat.shape
        
        # é‡å»º3D tensor
        all_features = np.zeros((n_samples, n_channels, n_times))
        all_labels = []
        
        for i, sample in enumerate(self.samples):
            all_features[i] = sample['features'].numpy()
            all_labels.append(sample['label'].item())
        
        # è½¬æ¢ä¸ºDataFrameæ ¼å¼
        rows = []
        for sample_idx in range(n_samples):
            for time_idx in range(n_times):
                row = {'sample_id': sample_idx, 'time_step': time_idx}
                for channel_idx in range(n_channels):
                    row[f'f_{channel_idx}'] = all_features[sample_idx, channel_idx, time_idx]
                rows.append(row)
        
        # åˆ›å»ºDataFrames
        self.feature_df = pd.DataFrame(rows)
        self.feature_df.set_index(['sample_id', 'time_step'], inplace=True)
        
        self.labels_df = pd.DataFrame({'label': all_labels})
        self.labels_df.index.name = 'sample_id'
        
        # è®¾ç½®å…¶ä»–å±æ€§
        self.feature_names = self.feature_df.columns.tolist()
        self.all_IDs = list(range(n_samples))
        self.max_seq_len = n_times
        self.class_names = ["æ—¥å¸¸ç”Ÿæ´»", "ç¤¾äº¤æƒ…æ„Ÿ", "ä¸“ä¸šæœåŠ¡"]
        
        if self.debug:
            print(f"\nè½¬æ¢ä¸ºDataFrameæ ¼å¼å®Œæˆ:")
            print(f"  feature_dfå½¢çŠ¶: {self.feature_df.shape}")
            print(f"  labels_dfå½¢çŠ¶: {self.labels_df.shape}")
            print(f"  ç‰¹å¾ç»´åº¦: {len(self.feature_names)}")
            print(f"  æ ·æœ¬æ•°: {n_samples}")'''
    def _convert_to_3class(self):
        """å°†39ç±»æ ‡ç­¾è½¬æ¢ä¸º3ç±»"""
        # ä½¿ç”¨ eeg_processor.py ä¸­çš„å‡½æ•°
        mapping_3cat = create_3category_mapping()
        
        # è·å–åŸå§‹æ ‡ç­¾
        original_labels = [sample['label'].item() for sample in self.samples]
        unique_labels = set(original_labels)
        self.original_num_classes = len(unique_labels)
        
        if self.debug:
            print(f"\nè½¬æ¢å‰çš„æ ‡ç­¾åˆ†å¸ƒ (39ç±»):")
            label_counts = Counter(original_labels)
            for label_id in sorted(label_counts.keys()):
                count = label_counts[label_id]
                percentage = count / len(original_labels) * 100
                print(f"  ç±»åˆ«{label_id}: {count} æ ·æœ¬ ({percentage:.1f}%)")
        
        # è½¬æ¢æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾
        new_samples = []
        converted_count = 0
        for sample in self.samples:
            original_label = sample['label'].item()
            new_label = mapping_3cat.get(original_label, -1)
            
            if new_label >= 0:  # åªä¿ç•™æœ‰æ•ˆæ˜ å°„
                sample['label'] = torch.tensor(new_label, dtype=torch.long)
                new_samples.append(sample)
                converted_count += 1
            elif self.debug:
                print(f"è­¦å‘Š: æ ‡ç­¾{original_label} æ— å¯¹åº”çš„3åˆ†ç±»æ˜ å°„")
        
        # æ›´æ–°æ ·æœ¬åˆ—è¡¨
        self.samples = new_samples
        
        # æ›´æ–°ç±»åˆ«æ•°ä¸º3
        self.num_classes = 3
        if hasattr(self, 'data_dict'):
            self.data_dict['num_classes'] = 3
        
        if self.debug:
            print(f"\nè½¬æ¢ç»“æœ:")
            print(f"  åŸå§‹æ ·æœ¬æ•°: {len(original_labels)}")
            print(f"  è½¬æ¢åæ ·æœ¬æ•°: {converted_count}")
            print(f"  æœ‰æ•ˆè½¬æ¢ç‡: {converted_count/len(original_labels)*100:.1f}%")
            print(f"  ç±»åˆ«è½¬æ¢: {self.original_num_classes}ç±» -> 3ç±»")
            
            # æ˜¾ç¤º3åˆ†ç±»åˆ†å¸ƒ
            new_labels = [sample['label'].item() for sample in self.samples]
            label_counts_3cat = Counter(new_labels)
            label_names = {0: "æ—¥å¸¸ç”Ÿæ´»", 1: "ç¤¾äº¤æƒ…æ„Ÿ", 2: "ä¸“ä¸šæœåŠ¡"}
            print(f"\n3åˆ†ç±»åˆ†å¸ƒ:")
            for label_id in sorted(label_counts_3cat.keys()):
                count = label_counts_3cat[label_id]
                percentage = count / len(self.samples) * 100
                name = label_names.get(label_id, f"æœªçŸ¥ç±»åˆ«{label_id}")
                print(f"  {name}({label_id}): {count} æ ·æœ¬ ({percentage:.1f}%)")
    
   
    """åŠ è½½EEGæ•°æ®ï¼Œè¿”å›3åˆ†ç±»"""
    def _load_eeg_data(self, data_dir):
        """åŠ è½½EEGæ•°æ®"""
        if self.debug:
            print(f"æ­£åœ¨åŠ è½½EEGæ•°æ®")
            print(f"  æ•°æ®ç›®å½•: {data_dir}")
            print(f"  JSONè·¯å¾„: {self.json_path}")
            print(f"  æœ€å¤§æ–‡ä»¶æ•°: {self.max_files}")
         # âœ… ä»selfä¸­è·å–subject_idså‚æ•°
        if hasattr(self, 'subject_ids'):
            subject_ids = self.subject_ids
        else:
            subject_ids = None
            if self.debug:
                print(f"  âš  è­¦å‘Š: æ²¡æœ‰subject_idså‚æ•°ï¼Œå°†å¤„ç†æ‰€æœ‰è¢«è¯•")

   

            # æ£€æŸ¥æ•°æ®ç›®å½•
            if not os.path.exists(data_dir):
                print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
                return None

            # æ£€æŸ¥JSONæ–‡ä»¶
            if not os.path.exists(self.json_path):
                print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {self.json_path}")
                return None

            try:
                data_dict = process_imagine_fif_data_with_label_mapping(
                    data_dir, 
                    self.json_path, 
                    self.max_files, 
                    self.debug
                )

                if data_dict is None:
                    print("âŒ process_imagine_fif_data_with_label_mapping è¿”å› None")
                    return None

                return data_dict

            except Exception as e:
                print(f"âŒ è°ƒç”¨process_imagine_fif_data_with_label_mappingå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return None

            # ç¡®ä¿æ˜¯3åˆ†ç±»
            if 'num_classes' in data_dict and data_dict['num_classes'] != 3:
                if self.debug:
                    print(f"åŸå§‹æ•°æ®æ˜¯ {data_dict['num_classes']} ç±»ï¼Œæ­£åœ¨è½¬æ¢ä¸º3åˆ†ç±»...")

                # å°†numeric_labelsè½¬æ¢ä¸º3åˆ†ç±»
                if 'numeric_labels' in data_dict:
                    original_labels = data_dict['numeric_labels']
                    # è½¬æ¢ä¸º3åˆ†ç±»
                    three_class_labels = convert_to_3category_labels(original_labels)

                    # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
                    valid_indices = [i for i, label in enumerate(three_class_labels) if label >= 0]

                    if len(valid_indices) > 0:
                        data_dict['numeric_labels'] = [three_class_labels[i] for i in valid_indices]
                        data_dict['input_features'] = data_dict['input_features'][valid_indices]
                        if 'text_labels' in data_dict:
                            data_dict['text_labels'] = [data_dict['text_labels'][i] for i in valid_indices]

                        data_dict['num_classes'] = 3
                        data_dict['sample_count'] = len(data_dict['numeric_labels'])

                        if self.debug:
                            print(f"è½¬æ¢ä¸º3åˆ†ç±»å®Œæˆ: {len(original_labels)} -> {len(valid_indices)} ä¸ªæ ·æœ¬")
                    else:
                        raise ValueError("è½¬æ¢ä¸º3åˆ†ç±»åæ— æœ‰æ•ˆæ ·æœ¬")

            return data_dict




